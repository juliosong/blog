---
title: "Another application of category theory in linguistics (Part 2)"
categories: [linguistics, mathematics]
tags: [category theory, generative grammar]
header:
  overlay_image: /assets/images/act2022.png
  show_overlay_excerpt: false
  image_description: "a Rubik's cube"
  caption: "credit: [Kenny Eliason](https://unsplash.com/@neonbrand?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/s/photos/smart?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)"
  teaser: /assets/images/act2022.png
toc: true
toc_sticky: true
---

[[Link to Part 1 of this post]({{ site.baseurl }}{% post_url 2022-07-15-another-application-of-category-theory-in-linguistics-part-1 %})]

The title of my ACT2022 submission is "Category theory in theoretical linguistics: A monadic semantics for root syntax." I have clarified the relationship between category theory and theoretical linguistics and the relevance of monadic semantics, but what is root syntax? I try to introduce its gist below, after which I turn to clarify the parallelism between its core technique, root categorization, and the writer monad.

## Root syntax
As I mentioned above, formal syntactic research at the current stage is regularly subatomic, in the sense that researchers treat word-internal structures as normal syntactic structures and derive them in the same way as they derive phrases and sentences. Root syntax, as an offshoot of TGG (recall that TGG stands for "transformational generative grammar," aka Chomskyan syntax), is the epitome of subatomic syntactic analysis and has been gaining popularity among generative linguists since the 1990s (after several seminal papers by [Morris Halle](https://en.wikipedia.org/wiki/Morris_Halle) and [Alec Marantz](https://as.nyu.edu/faculty/alec-marantz.html) on the "[distributed morphology](https://en.wikipedia.org/wiki/Distributed_morphology)" framework).

Long story short, in root syntax, content words are routinely broken down to a purely grammatical/functional part, called the "categorizer," and a purely lexical/idiosyncratic part, called the "root." For example, the noun *dog* has the syntactic structure [<sub>N</sub> N ‚àöDOG]. The same root could be categorized in a different way and thereby get a different interpretation, such as [<sub>V</sub> V ‚àöDOG], which corresponds to the verb *dog*. Of course, which categorizer-root combination can trigger what interpretation (possibly null) is a matter of language-specific lexicalization and conventionalization---but that's just how lexical semantics is!

NB the term "root" here is used in a quite different way from the [more familiar notion](https://en.wikipedia.org/wiki/Root_(linguistics)) in morphology. A root in root syntax is an abstract theoretical construct with no concrete sound or meaning. Metaphorically, it is like a linguistic nebula that can be shaped in multiple ways, and the categorizer serves to "mold" its shape, thus activating a concrete sound-meaning pair based off of it. Alternatively, one could imagine a root as an entire space of potential "word-sized" interpretations, and when it is categorized, a particular interpretation is picked from that space.

Back to my point about interdisciplinary communication: Root syntax is nothing new in linguistics---it was already there in the 1990s as mentioned above. On the other hand, the "mysterious" intergration of lexical and compositional semantics is not a puzzle for linguists alone either. Apparently, it is also a problem in NLP, and [Coecke et al.'s (2010 et seq.)](https://arxiv.org/abs/1003.4394) work (also using category theory) deals with pretty much the same issue from a different angle. I touched on a possible parallelism between the two angles---namely, Coecke et al.'s NLP angle and my theoretical linguistic angle---in my third preliminary publication mentioned in Part 1 and plan to further explore that possibility in my future research.

## Syntax-semantics interface
Now that I've clarified all words in the title of my ACT2022 submission except one: the preposition "for." Indeed, why should we bother to give root syntax a semantics? After all, linguists have been using root syntax to model various empirical phenomena since three decades ago, and it seems that they have been doing just fine without bringing compositionality into the picture. Besides, root syntax is meant to be a formal syntactic rather than a formal semantic theory anyway, so why bother?

As far as I'm concerned, we can't completely ignore compositional semantics when doing root syntax because linguistic theories and the assumptions they carry all make claims, either explicitly or implicitly, about how human language works in general. And vice versa, general principles of how human language works always interact with specific mechanisms of specific theories. As for our case at hand, this means that the root-categorization structure, being part of the formal syntactic structure, ought to be normally included in the syntax-to-semantics mapping, as required by [Frege's Principle](https://en.wikipedia.org/wiki/Principle_of_compositionality). As such, the lack of a compositional semantics for root syntax in effect creates a hole in the syntax-semantics interface.

Moreover, due to the particular way roots are defined in root syntax (i.e., no fixed interpretation), this hole can't be easily filled by usual logical tools. To illustrate, if one decomposes $\lambda x. \mathrm{dog}(x)$ (i.e., the denotation of the noun "dog") into a skeletal part $\lambda x. \mathrm{N}(x)$, which merely specifies a nominal denotation template, and an idiosyncratic part $\mathrm{dog}$, there's an immediate type-level dilemma: What is the type of $\mathrm{dog}$? And how to compose the two parts back to the ordinary first-order predicate? One solution that has been suggested in the literature is that $\mathrm{dog}$ is a first-order predicate itself, and the mode of composition between the two parts is conjunction. However, that seemingly kicks off an infinite loop, for $\mathrm{dog}$ on the first-order predicate view is equivalent to $\lambda x. \mathrm{dog}(x)$! There's still other potential solutions based on this "direct composition" view, but as I demonstrated in my [2021 proceedings paper](https://www.juliosong.com/doc/Song2021LENLS18.pdf), all such "ordinary" solutions hit deadends, especially in the face of semigrammatical words.

It was the above dilemma that had driven me to look for solutions outside of linguists' comfort zone, or so to speak, "from a different universe"---somewhere that looks sufficiently logical yet is not the particular universe named "logic." And as you can guess, I landed in the universe of category theory (which wasn't entirely accidental, of course, as I had always had a gut feeling that CT could be helpful in this regard). In my 2021 paper, I actually listed two category-theoretic approaches, one based on equivalent morphisms and diagram chasing in a [topos](https://en.wikipedia.org/wiki/Topos) (which is just $\mathbf{Set}$ in our domain of application), and the other based on monadic composition (using the writer monad). But in my ACT2022 submission, I only focused on the second approach due to time and space limitations. I plan to further develop the first approach too sometime in the future. ‚ò∫Ô∏è

## Monad
Readers familiar with functional programming probably already figured out how I would use the monad tool to compose the categorizer and the root. Indeed, the conceptual parallelism between root syntax and monadic composition is surprisinly intuitive and easy to understand (to the extent that my ACT2022 submission may look a bit too "mathematically naive" üòÇ). Could that be an indication that our mind works monadically under the hood? Maybe, because the monad tool has also long been used to model [modality issues](https://ncatlab.org/nlab/show/modal%20type%20theory#RelationToMonads), and we also know independently from linguistics that the outer packaging of human language is always modal or intensional---generative linguists nowadays use a "speech act shell" to reflect this in the syntactic representation (check out the [cool work](http://martinawiltschko.com/research-2/interactionallanguage/) of Martina Wiltschko if you're curious üòâ).

The parallelism between root syntax and monadic semantics can be semiformally summarized as follows:
> Take a writer monad $T$ that wraps a pure value $x$ in an ordered pair $\langle x, m\rangle$, whose second component $m$ is a monoid element. We can view $x$ as the purely functional side of a content/semilexical word's meaning (i.e., the categorizer) and view $m$ as the purely idiosyncratic side thereof (i.e., the root).

As usual, the pure value $x$ could also be a pure function $f$. And while monadic values compose together per normal grammatical rules, the second slot of the monadic pair serves as a log area recording which root's "capsule of idiosyncrasies" supports which pure function. This information is accumulated in the compositional procedure (by means of the monoid) but only made use of when compositional and noncompositional meaning components are finally integrated, to the effect that when we understand a noun like "dog" or a verb like "run," what we really understand is:
- an `entity` supported by the idiosyncrasy capsule üíä‚àöDOG, or
- an `eventuality` supported by the idiosyncrasy capsule üíä‚àöRUN.

And our understanding of semigrammatical words follows exactly the same pattern, as in
- the Chinese classifier *mian4* (for thin, flat objects with a useful surface, such as mirrors and walls):üëâ<br> a mass-to-count conversion `function` supported by the idiosyncrasy capsule üíä‚àöMIAN, and
- the Vietnamese negation particle *ƒë√©o* (highly vulgar, literally 'the fuck' and borrowed from Cantonese *diu2*, which is a vulgar way to say 'penis'):üëâ<br>
a boolean $1\rightarrow 0$ `function` supported by the idiosyncrasy capsule üíä‚àöƒê√âO.

Again, this unification of content and semigrammatical words in the monadic semantics neatly mirrors a similar unification in some recent TGG works---including my own PhD work, where I called the unification "generalized root syntax."

## Categorical setting
While my ACT2022 submission is mostly based on my 2021 paper as mentioned above, it does include at least one major update---I left the categorical setting behind the "root support" monad vague last year but fleshed it out this year. Again, the setting I choose may appear overly simple to hardcore category theorists, because I have basically stuck to the good old [cartesian closed category](https://en.wikipedia.org/wiki/Cartesian_closed_category), more exactly to the most "mundane" instantiation thereof: $\mathbf{Set}$.

The truth is, I have deliberately avoided overcomplicating the syntax category (NB here "category" is used in the CT sense) in my work, not because I dislike more complex categorical structures---among others, I find the [compact closed](https://ncatlab.org/nlab/show/compact+closed+category) structure in Lambekian syntax (aka [pregroup grammar](https://en.wikipedia.org/wiki/Pregroup_grammar)) beautiful and super useful for NLP purposes, but I don't really think the extra stuff in there (especially the directional part) is suitable for an adequate theory of human language grammar qua a natural phenomenon, at least not from a linguist-linguist's perspective.

As I mentioned in Part 1 of this post, most existing works in categorical linguistics are focused on syntax/semantics, including my own work. However, a widely accepted hypothesis about human language grammar among linguists is its modularity. In the TGG framework, in particular, where it is assumed that the syntax module precedes the phonology and the semantics module (which is related to [the autonomy of syntax](https://en.wikipedia.org/wiki/Autonomy_of_syntax)), things that happen in syntax ought to have both phonological and semantic effect. In other words, if an operation only has phonological effect, then it should be placed in the phonology module. Word order--related (aka linearization) operations are a typical example, as reflected in the following quote from Berwick & Chomsky's 2016 monograph [*Why Only Us*](https://mitpress.mit.edu/books/why-only-us):
> The past sixty years of research into generative grammar has uncovered several basic, largely uncontroversial, principles about human language. Human language syntactic structure has at least three key properties...: (1) human language syntax is hierarchical and is blind to considerations of linear order, with linear ordering constraints reserved for externalization; .... (Berwick & Chomsky 2016:7--8)

An immediate consequence of this principle is that any adequate modeling of human language grammar must have a separate phonological component. Again, that is an uncontroversial tenet held by linguists as far as I'm concerned. Due to the modularity of human language grammar and the phonological nature of word order linearization, I have avoided directional syntactic operations in the categorical setting of my work.

In addition, I don't think the [resource-oriented thinking](https://en.wikipedia.org/wiki/Linear_logic#The_resource_interpretation) (i.e., once you consume a premise in syntactic derivation, it's gone) that has motivated the adoption of [linear logic](https://en.wikipedia.org/wiki/Linear_logic) in natural language modeling in some recent works (including Asudeh & Giorgolo 2020) is fully compatible with how things are done in theoretical linguistics either, at least in TGG. A most important syntactic operation throughout the history of TGG is [movement](https://en.wikipedia.org/wiki/Syntactic_movement), which defines the "T" (= transformational) part of its name. Every movement step in the TGG syntactic derivation gives rise to two copies of the same syntactic object (linguistic sense!), and in cases of "long-distance movement," multiple copies are created in the course of the derivation (with only the structurally highest copy being pronounced). Considering the lack of strict abidance by the resource-oriented thinking in TGG, I have adopted the good old classical logic rather than linear logic in my work so far, which in turn makes a fully general monoidal category unnecessary in the foundation of my monadic semantics---$\mathbf{Set}$ is already enough.

Another issue I discussed in my 2022 work but not in my 2021 work is the "best candidate" nature of the monadic approach to root syntax semantics, especially in contrast with a potential alternative approach based on the [applicative functor](https://en.wikipedia.org/wiki/Applicative_functor) from functional programming. But since this blog is already long enough, and that discussion is quite convoluted both categorically and linguistically, I'll stop here and refer interested readers to my [extended abstract](https://www.juliosong.com/doc/Song2022ACTabstract.pdf) for a preview of my argumentation, a full version of which will be included in an upcoming paper of mine (I must work harder üòù).

## Conclusion
In this two-part blogpost, I have gently (and quite verbosely) introduced the disciplinary background and some behind-the-scene thinking for my use of the category-theoretic concept monad in the composition semantics I have developed for root syntax, a currently popular branch of transformational generative grammar. Since root syntax, in its generalized form, unifies content words and semigrammatical words in subatomic syntactic derivation, the monadic semantics is actually highly relevant for the syntax-semantics interface, to the extent that wherever there is a content/semigrammatical word, there is monadic composition, hence the slogan at the beginning of the post: If we take lexical decomposition in morphosyntax seriously, then monadic composition is everywhere in human language semantics!

Thanks for reading till the end of this very long blogpost! And if you are still curious about my work, feel free to check out my [extended abstract](https://www.juliosong.com/doc/Song2022ACTabstract.pdf) and [e-poster](https://www.juliosong.com/doc/act2022poster/poster.html), where there's a little more formal discussion. 

== The End ==
